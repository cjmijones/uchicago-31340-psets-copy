{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set we are going to make use of `pandas` to analyze the effect of a fictuous experiment I have added to a data set. The data will be using is the sample data provided by Yelp. The goal is to familiarize ourselves with working with such datasets.\n",
    "\n",
    "[Download notebook :fontawesome-solid-download:](../hw-quasi.ipynb){: .md-button .md-button--primary download=\"hw-quasi.ipynb\"}\n",
    "\n",
    "The original data is available here: [Yelp data](https://www.yelp.com/dataset/download).\n",
    "However for this homework you will have to use the data I constructed from the original sample. You can download such file here: \n",
    "\n",
    " - **homework data**: [hw-yelp.tar.gz](http://econ21340.lamadon.com/hw-yelp.tar.gz) (~2.6Go)\n",
    "\n",
    "Note that on windows you can use [7zip](https://www.7-zip.org/download.html) to uncompress that file. On OSX and linux you can simply use `tar -cxvf hw-yelp.tar.gz`\n",
    "\n",
    "In the data I have introduced an experiment. The back sotry is that Yelp rolled out a new interface for a randomly select group of users. These uses were randomly selected among users that posted a review in the month of January 2010. The `id` of these users in listed in the `yelp_academic_dataset_review_treatment.json` file present in the archive.\n",
    "\n",
    "For this group of user a the new website interface was put in place on February 1st 2010. As a Yelp employee you are asked to analyze the impact of a new app. The company is interested in the effect on user engagement which is measured by rating activity. We will focus on the number of ratings.\n",
    "\n",
    "In this homework we will cover:\n",
    " 1. loading large data using streaming/chunks, learn about json\n",
    " - working with date in pandas\n",
    " - analyze randomly assigned treatment\n",
    " - construct comparable control group\n",
    " - analyze at the level of randomization\n",
    " \n",
    "some useufl links:\n",
    " - [tutorial on dates in pandas](https://pbpython.com/pandas-grouper-agg.html)\n",
    " - [pandas documentation on reshaping](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html)\n",
    " - [yelp data documentation](https://www.yelp.com/dataset/documentation/main)\n",
    "\n",
    "We start with a simple list of imports, as well as defining the path to the file we will be using. Please update the paths to point to the correct location on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T15:45:45.448679Z",
     "start_time": "2020-05-28T15:45:45.393214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/connorjones/mystuff/uchicago-31340-psets/Quasi_Pset/yelp_academic_dataset_review_experiment.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_review = os.path.expanduser(\"~/mystuff/uchicago-31340-psets/Quasi_Pset/yelp_academic_dataset_review_experiment.json\")\n",
    "file_treatment = os.path.expanduser(\"~/mystuff/uchicago-31340-psets/Quasi_Pset/yelp_academic_dataset_review_treatment.json\")\n",
    "\n",
    "def file_len(fname):\n",
    "    \"\"\" Function which efficiently computes the number of lines in file\"\"\"\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "file_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are already familiar with the following section, this is the code that loads my solution. Since you don't have the file, this part of the code won't work for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading the yelp review data\n",
    "\n",
    "The data is stored in `json` format. This is a widely used format to store structured data. See [here](https://www.w3schools.com/python/python_json.asp) for working with json in general in python.\n",
    "\n",
    "The data itself is quite large, hence we are going to use the `chunksize` argument of the `read_json` function of `pandas`. You can of course try for your self to directly load the data by using `pd.read_json(file_review)`, this however might take a while!\n",
    "\n",
    "In the following section I provide a code example that loads the business information using chunks of size `100,000`. The code **contains a few errors**. Use the data documentation (using the link in the intro) to fix the code a load the data. The code also drops variables which will be very needed and keep some others that are just going to clutter your computer memory. Again, look at the documentation and at the questions ahead to keep the right set of variables.\n",
    "\n",
    "\n",
    "Note how the code first compute the length of the file \n",
    "\n",
    "```python\n",
    "size = 100000\n",
    "review = pd.read_json(filepath, lines=True,\n",
    "                      dtype={'review_id':str,\n",
    "                             'user_id':float,\n",
    "                             'business_id':str,\n",
    "                             'stars':int,\n",
    "                             'date':str,\n",
    "                             'text':float,\n",
    "                             'useful':int,\n",
    "                             'funny':str,\n",
    "                             'cool':int},\n",
    "                      chunksize=size)\n",
    "\n",
    "chunk_list = []\n",
    "for chunk_review in tqdm.tqdm(review,total=  np.ceil(file_len(filepath)/size )  ):\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_review = chunk_review.drop(['review_id','date'], axis=1)\n",
    "    chunk_list.append(chunk_review)\n",
    "\n",
    "df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)\n",
    "```\n",
    "\n",
    "The following runs my version of the code, it takes around 2 minutes on my laptop. I show you a few of the columns that I chose to extract. In particular, you can check that you get the right row count of `7998013`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_question1(filepath):\n",
    "    size = 100000\n",
    "    review = pd.read_json(filepath, lines=True,\n",
    "                          dtype={'review_id':str,\n",
    "                                 'user_id':float,\n",
    "                                 'business_id':str,\n",
    "                                 'stars':int,\n",
    "                                 'date':str,\n",
    "                                 'text':float,\n",
    "                                 'useful':int,\n",
    "                                 'funny':str,\n",
    "                                 'cool':int},\n",
    "                          chunksize=size)\n",
    "\n",
    "    chunk_list = []\n",
    "    for chunk_review in tqdm.tqdm(review,total=  np.ceil(file_len(filepath)/size )  ):\n",
    "        chunk_review = chunk_review.drop(['text','useful','funny','cool'], axis=1)\n",
    "        chunk_list.append(chunk_review)\n",
    "    \n",
    "    df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T15:48:24.404316Z",
     "start_time": "2020-05-28T15:45:49.033902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80.0 [01:45<00:00,  1.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n",
       "      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n",
       "      <td>2015-04-15 05:21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n",
       "      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n",
       "      <td>2013-12-07 03:16:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n",
       "      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n",
       "      <td>2015-12-05 03:18:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n",
       "      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n",
       "      <td>2011-05-27 05:30:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998008</th>\n",
       "      <td>LAzw2u1ucY722ryLEXHdgg</td>\n",
       "      <td>6DMFD3BRp-MVzDQelRx5UQ</td>\n",
       "      <td>2019-12-11 01:07:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998009</th>\n",
       "      <td>gMDU14Fa_DVIcPvsKtubJA</td>\n",
       "      <td>_g6P8H3-qfbz1FxbffS68g</td>\n",
       "      <td>2019-12-10 04:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998010</th>\n",
       "      <td>EcY_p50zPIQ2R6rf6-5CjA</td>\n",
       "      <td>Scmyz7MK4TbXXYcaLZxIxQ</td>\n",
       "      <td>2019-06-06 15:01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998011</th>\n",
       "      <td>-z_MM0pAf9RtZbyPlphTlA</td>\n",
       "      <td>lBuAACBEThaQHQGMzAlKpg</td>\n",
       "      <td>2018-07-05 18:45:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998012</th>\n",
       "      <td>nK0JGgr8aO4mcFPU4pDOEA</td>\n",
       "      <td>fiA6ztHPONUkmX6yKIXyHg</td>\n",
       "      <td>2019-12-07 00:29:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7998013 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      review_id                 user_id                date\n",
       "0        xQY8N_XvtGbearJ5X4QryQ  OwjRMXRC0KyPrIlcjaXeFQ 2015-04-15 05:21:16\n",
       "1        UmFMZ8PyXZTY2QcwzsfQYA  nIJD_7ZXHq-FX8byPMOkMQ 2013-12-07 03:16:52\n",
       "2        LG2ZaYiOgpr2DK_90pYjNw  V34qejxNsCbcgD8C0HVk-Q 2015-12-05 03:18:11\n",
       "3        i6g_oA9Yf9Y31qt0wibXpw  ofKDkJKXSKZXu5xJNGiiBQ 2011-05-27 05:30:52\n",
       "4        6TdNDKywdbjoTkizeMce8A  UgMW8bLE0QMJDCkQ1Ax5Mg 2017-01-14 21:56:57\n",
       "...                         ...                     ...                 ...\n",
       "7998008  LAzw2u1ucY722ryLEXHdgg  6DMFD3BRp-MVzDQelRx5UQ 2019-12-11 01:07:06\n",
       "7998009  gMDU14Fa_DVIcPvsKtubJA  _g6P8H3-qfbz1FxbffS68g 2019-12-10 04:15:00\n",
       "7998010  EcY_p50zPIQ2R6rf6-5CjA  Scmyz7MK4TbXXYcaLZxIxQ 2019-06-06 15:01:53\n",
       "7998011  -z_MM0pAf9RtZbyPlphTlA  lBuAACBEThaQHQGMzAlKpg 2018-07-05 18:45:21\n",
       "7998012  nK0JGgr8aO4mcFPU4pDOEA  fiA6ztHPONUkmX6yKIXyHg 2019-12-07 00:29:55\n",
       "\n",
       "[7998013 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review = jones_question1(file_review)\n",
    "df_review['date'] = pd.to_datetime(df_review.date) # convert the date string to an actual date\n",
    "df_review[['review_id','user_id','date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first plot of the data\n",
    "\n",
    "Next, to get a sense of the data, we plot the user engagement over time. For this I ask you to plot the log number of reviews per month using our created data. \n",
    "\n",
    "To get to the result I recommend you look into either the `resample` menthod or the `grouper` method. If you are not too familiar with them, I added a link at the top to a great tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_question2(dataframe):\n",
    "    df_test = dataframe\n",
    "    df_test = df_test.set_index('date').resample('M')[\"user_id\"].agg('count')\n",
    "    df_test = df_test.to_frame()\n",
    "\n",
    "    y = np.log(df_test.user_id + 1)\n",
    "    x = df_test.index\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"log user_id\")\n",
    "    plt.title(\"log number of reviews per months\")\n",
    "    plt.show()\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T15:50:27.983702Z",
     "start_time": "2020-05-28T15:49:58.341687Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jones_question2(df_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A randomized experiment\n",
    "\n",
    "We now want to extract our experimental data from our large data set. Given the random assignment we are going to compare the treated group to simply everyone else in the data. In this exercice, we are interested in the effect of the policy overt time. We are then going to look at the log number of reviews in each of the month around the introduction of the interface change.\n",
    "\n",
    "I would like for you to do the following:\n",
    " 1. extract the list of treated individuals from the provided file\n",
    " 2. attach the treated status to each observation in the data, you can use `eval` or a merge.\n",
    " 3. plot the log number of reviews per month in the treatment and in the control group. \n",
    " 4. given that the treatment status was randomized, the picture should look a bit surpising, please explain what you would have expected to see.\n",
    " \n",
    "Here is the plot I get, try to reproduce it as closely as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_question3(total, treat):\n",
    "    df_treatment = pd.read_json(treat, lines=True)\n",
    "    df_treatment = df_treatment.rename(columns={0: \"user_id\"})\n",
    "    df_treatment['user_treat'] = 1\n",
    "\n",
    "    ust = pd.read_json(treat, lines=True)\n",
    "    ust_list = list(ust[0])\n",
    "    \n",
    "    df_merge = total\n",
    "    df_merge = pd.merge(df_merge, df_treatment, how='outer')\n",
    "    df_merge['user_treat'] = df_merge['user_treat'].fillna(0)\n",
    "        \n",
    "    df_test = total\n",
    "    df_test = df_test.set_index('date').resample('M')[\"user_id\"].agg('count')\n",
    "    df_test = df_test.to_frame()\n",
    "    \n",
    "    df_subgroup = df_merge[df_merge['user_treat'] == 1]\n",
    "    df_subgroup = df_subgroup.set_index('date').resample('M')[\"user_id\"].agg('count')\n",
    "    df_subgroup = df_subgroup.to_frame()\n",
    "    \n",
    "    y = np.log(df_test.user_id+1)\n",
    "    x = df_test.index\n",
    "    z = np.log(df_subgroup.user_id+1)\n",
    "    \n",
    "    plt.plot(x,y, label=\"Total\")\n",
    "    plt.plot(x,z, label=\"Treat\")\n",
    "    plt.title(\"log number of reviews per months treated and untreated\")\n",
    "    plt.show()\n",
    "    \n",
    "    return ust_list, df_merge;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:27:56.337638Z",
     "start_time": "2020-05-28T16:27:31.211000Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_local returns all entries with a column treated, user_treat is the list of treated user_id\n",
    "user_treat,df_local = jones_question3(df_review, file_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:28:04.521201Z",
     "start_time": "2020-05-28T16:28:04.418172Z"
    }
   },
   "source": [
    "### Question 3.4\n",
    "The picture is surprising because we selected a random pool of users for the treatment which would be expected to produce a representive sample of the overall user data.  However, in the picture there appears to be a shift around 2010 when the treatment group user numbers appear to start reverting to a different mean than the total sample.  This suggests that there was an error when randomly selecting the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing comparaison group\n",
    "\n",
    "We clearly created some issues in the way we analyzed our sample. In this section we are going to use a more comparable group. \n",
    "\n",
    " 1. using the criteria descriged in the intro, construct the original set of users from which the treatment group was selected. \n",
    " - extracts the users from the this group wich are not in the treatment group, this will be our control group.\n",
    " - using this new control group, plot the log number of reviews in each **quarter** for treatment and control\n",
    " - finally plot the outcome in difference, however make sure to remove the log-number of individual from each group to plot the log number of reviews per user, overwise your intercept won't be around 0!\n",
    " \n",
    "Here are the plots I got: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_question4(df_local, ust):\n",
    "    control = df_local[df_local.date.dt.year == 2010]\n",
    "    control = control[control.date.dt.month == 1]\n",
    "    control = control[control['user_treat'] == 0]\n",
    "    syn_con = pd.DataFrame(control.user_id.unique())\n",
    "    syn_con = syn_con.rename(columns={0: \"user_id\"})\n",
    "    syn_con['control'] = 1\n",
    "    merge = pd.merge(df_local, syn_con, how='outer')\n",
    "    merge['control'] = merge['control'].fillna(0)\n",
    "    merge\n",
    "\n",
    "    control = merge[merge['control'] == 1]\n",
    "    control = control.set_index('date').resample('Q')[\"user_id\"].agg('count')\n",
    "    control = control.to_frame()\n",
    "\n",
    "    treat = merge[merge['user_treat'] == 1]\n",
    "    treat = treat.set_index('date').resample('Q')[\"user_id\"].agg('count')\n",
    "    treat = treat.to_frame()\n",
    "\n",
    "    y = np.log(control.user_id+1)\n",
    "    x = control.index\n",
    "    a = treat.index\n",
    "    z = np.log(treat.user_id+1)\n",
    "\n",
    "    plt.plot(x,y, label = \"Control\")\n",
    "    plt.plot(a,z, label = \"Treat\")\n",
    "    plt.title(\"log number of reviews per months treated and untreated\")\n",
    "    plt.show()\n",
    "    \n",
    "    c = z - np.log(1500)\n",
    "    d = y - np.log(3346)\n",
    "    \n",
    "    e = c - d\n",
    "    \n",
    "    plt.plot(a,e)\n",
    "    plt.title(\"diff in reviews per user: treat - control\")\n",
    "    plt.show()\n",
    "    \n",
    "    final = merge[(merge['control'] == 1) | (merge['user_treat'] == 1)]\n",
    "    return final;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:28:24.330462Z",
     "start_time": "2020-05-28T16:28:19.066214Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_local = jones_question4(df_local,user_treat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using activity per user and time\n",
    "\n",
    "We are now interested in conducting some inference on our results. However we remember that the level of randomization is the `user` not the `review`. Hence we now decide to construct observation at the `(user,year)` level. We decide to use years instead of months because the probability at the month level is too low.\n",
    "\n",
    " 1. Construct a DataFrame with all `(user,year)` pairs and a column called `post` which is equal to 1 if the user posted in that month and 0 if he didn't. To construct such dataframe I used the `pd.MultiIndex.from_product` function, but one could use a `merge` instead.\n",
    " 2. Use this newly created DataFrame to plot the level for each group and each, and to plot the difference between the two.\n",
    " \n",
    "Here are the plots I constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_question5(df_local, ust):\n",
    "    df_l2 = df_local\n",
    "    df_l2['year'] = df_l2.date.dt.year\n",
    "    df_l2['post'] = 1\n",
    "    df_l2 = df_l2.drop([\"date\", \"business_id\", \"stars\"], axis=1)\n",
    "    df_l2\n",
    "\n",
    "    control = df_local[df_local.date.dt.year == 2010]\n",
    "    control = control[control.date.dt.month == 1]\n",
    "    syn_con = pd.DataFrame(control.user_id.unique())\n",
    "    syn_con = syn_con[0].tolist()\n",
    "\n",
    "    test = df_local.date.dt.year.unique()\n",
    "    test = list(test)\n",
    "\n",
    "    df_l3 = pd.MultiIndex.from_product([test, syn_con], names=['year','user_id']).to_frame(index=False)\n",
    "    df_l3\n",
    "\n",
    "    store = pd.merge(df_l2, df_l3, on=[\"user_id\", \"year\"], how=\"right\")\n",
    "    store['post'] = store['post'].fillna(0)\n",
    "\n",
    "    subgroup = store.drop_duplicates([\"user_id\", \"year\"])\n",
    "\n",
    "    test = subgroup[subgroup.user_treat == 1]\n",
    "    test2 = subgroup[subgroup.user_treat == 0]\n",
    "\n",
    "    alpha = test.year.value_counts().to_frame()\n",
    "    beta = test2.year.value_counts().to_frame()\n",
    "\n",
    "    alpha = alpha / 1500\n",
    "    beta = beta / 3346\n",
    "    alpha['active'] = alpha['year']\n",
    "    alpha['year'] = alpha.index\n",
    "\n",
    "    beta['active'] = beta['year']\n",
    "    beta['year'] = beta.index\n",
    "\n",
    "    alpha = alpha.sort_values('year')\n",
    "    beta = beta.sort_values('year')\n",
    "\n",
    "    a = alpha.year\n",
    "    b = alpha.active\n",
    "\n",
    "    c = beta.year\n",
    "    d = beta.active\n",
    "\n",
    "    plt.plot(a,b)\n",
    "    plt.plot(c,d)\n",
    "    plt.show()\n",
    "    \n",
    "    x = b - d\n",
    "    plt.plot(a,x)\n",
    "    plt.show()\n",
    "    \n",
    "    return subgroup;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_question5(df_local, ust):\n",
    "    df_l2 = df_local\n",
    "    df_l2['year'] = df_l2.date.dt.year\n",
    "    df_l2['post'] = 1\n",
    "    df_l2 = df_l2.drop([\"date\", \"business_id\", \"stars\"], axis=1)\n",
    "    df_l2\n",
    "\n",
    "    control = df_local[df_local.date.dt.year == 2010]\n",
    "    control = control[control.date.dt.month == 1]\n",
    "    syn_con = pd.DataFrame(control.user_id.unique())\n",
    "    syn_con = syn_con[0].tolist()\n",
    "\n",
    "    test = df_local.date.dt.year.unique()\n",
    "    test = list(test)\n",
    "\n",
    "    df_l3 = pd.MultiIndex.from_product([test, syn_con], names=['year','user_id']).to_frame(index=False)\n",
    "    df_l3\n",
    "\n",
    "    store = pd.merge(df_l2, df_l3, on=[\"user_id\", \"year\"], how=\"right\")\n",
    "    store['post'] = store['post'].fillna(0)\n",
    "\n",
    "    subgroup = store.drop_duplicates([\"user_id\", \"year\"])\n",
    "\n",
    "    test = subgroup[subgroup.user_treat == 1]\n",
    "    test2 = subgroup[subgroup.user_treat == 0]\n",
    "\n",
    "    alpha = test.year.value_counts().to_frame()\n",
    "    beta = test2.year.value_counts().to_frame()\n",
    "\n",
    "    alpha = alpha / 1500\n",
    "    beta = beta / 3346\n",
    "    alpha['active'] = alpha['year']\n",
    "    alpha['year'] = alpha.index\n",
    "\n",
    "    beta['active'] = beta['year']\n",
    "    beta['year'] = beta.index\n",
    "\n",
    "    alpha = alpha.sort_values('year')\n",
    "    beta = beta.sort_values('year')\n",
    "\n",
    "    a = alpha.year\n",
    "    b = alpha.active\n",
    "\n",
    "    c = beta.year\n",
    "    d = beta.active\n",
    "\n",
    "    plt.plot(a,b)\n",
    "    plt.plot(c,d)\n",
    "    plt.show()\n",
    "    \n",
    "    x = b - d\n",
    "    plt.plot(a,x)\n",
    "    plt.show()\n",
    "\n",
    "    return subgroup;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:28:30.094257Z",
     "start_time": "2020-05-28T16:28:28.203523Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_local_user = jones_question5(df_local,user_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct confidence inference\n",
    "\n",
    "In this final question our goal is to add some inference to our plot. We are going to simply use the asymptotic variance implied by the OLS formula. Do the following:\n",
    "\n",
    " 1. create a function that will take a dataframe containing the columns `post` and `treat` and returns the OLS estimate of `post` on `treat` together with the estimate of the variance of that estimate (Remember that in this simple case $\\hat{\\beta} = cov(y,x)/var(x)$ and that teh variance is $\\sigma^2_\\epsilon/(n \\cdot var(x))$. Return the results as a new dataframe with one row and 2 columns.\n",
    " 2. apply your function to your data from question 4 within eave `year` (you can do that using `pd.Grouper(freq='Y',key='date')` within a `groupby` and use the `apply` method.\n",
    " 3. use your grouped results to plot the mean together with their 95% asymptotic conf interval\n",
    " 4. comment on the results, in particular on date before the start of the experiment.\n",
    " \n",
    "I report the plot I got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "def question6_helper(df_local_user):\n",
    "    result = sm.ols(formula=\"post ~ user_treat\", data=df_local_user).fit()\n",
    "    ols_est = pd.DataFrame({'beta_hat': [result.params[1]], 'var_hat': [result.bse[1]**2], \n",
    "                            'sd_hat': [result.bse[1]],\n",
    "                            '95_CI' : [1.96*result.bse[1]]})\n",
    "    return ols_est;\n",
    "\n",
    "def beta_by_year(df_local_user):\n",
    "    \n",
    "    ust = pd.DataFrame(df_local_user[df_local_user['user_treat'] == 1]['user_id'].unique())\n",
    "    ust = ust.rename(columns={0: \"user_id\"})\n",
    "    ust['user_treat'] = 1\n",
    "\n",
    "    final = df_local_user\n",
    "    final = pd.merge(final, ust, on=\"user_id\", how=\"left\")\n",
    "    final['user_treat_x'] = final['user_treat_x'].fillna(0)\n",
    "    final['user_treat_y'] = final['user_treat_y'].fillna(0)\n",
    "    final = final.assign(user_treat = final['user_treat_x'] + final['user_treat_y']).drop(['user_treat_x', 'user_treat_y', 'control'], axis=1)\n",
    "    \n",
    "    \n",
    "    yr_df = pd.DataFrame(final.year.unique())\n",
    "    yr_df = yr_df.rename(columns={0: \"year\"})\n",
    "    yr_df = yr_df.sort_values('year')\n",
    "    \n",
    "    past = pd.DataFrame({'beta_hat' : [], 'var_hat' : [], 'sd_hat' : [], '95_CI' : []})\n",
    "    \n",
    "    for x in yr_df['year']:\n",
    "        holder = final[final.year == x]\n",
    "        ols = question6_helper(holder)\n",
    "        ols_est = pd.concat([past,ols])\n",
    "        past = ols_est\n",
    "        \n",
    "    return ols_est.set_index(yr_df['year']+1);\n",
    "\n",
    "def jones_question6(df_local_user):\n",
    "    subgroup = df_local_user\n",
    "    \n",
    "    test = subgroup[subgroup.user_treat == 1]\n",
    "    test2 = subgroup[subgroup.user_treat == 0]\n",
    "\n",
    "    alpha = subgroup[(subgroup.user_treat == 1) & (subgroup.post == 1)]\n",
    "    beta = subgroup[(subgroup.user_treat == 0) & (subgroup.post == 1)]\n",
    "\n",
    "    gold = pd.DataFrame(alpha.groupby('year')['post'].value_counts())\n",
    "    gold = gold.rename(columns={'post': \"treat_activity\"})\n",
    "    gold['control_activity'] = beta.groupby('year')['post'].value_counts()\n",
    "    gold['control_activity'] = gold['control_activity'] / 3346.0\n",
    "    gold['treat_activity'] = gold['treat_activity'] / 1500\n",
    "    gold.index = gold.index.droplevel(1) + 1\n",
    "\n",
    "    ret = pd.merge(beta_by_year(df_local_user), gold, on=\"year\")\n",
    "    \n",
    "    graph = ret\n",
    "\n",
    "    x = graph.index\n",
    "    y = graph.treat_activity\n",
    "    z = graph.control_activity\n",
    "\n",
    "    w = y - z\n",
    "    ret['diff'] = w\n",
    "    \n",
    "    lower_err = ret['diff'] - ret['95_CI']\n",
    "    upper_err = ret['diff'] + ret['95_CI']\n",
    "    asym_err = [lower_err, upper_err]\n",
    "    \n",
    "#     errorbar(x, y, yerr=error, fmt='-o')\n",
    "   \n",
    "    plt.errorbar(x, w, yerr=ret['95_CI'])\n",
    "    plt.show()    \n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:29:12.004387Z",
     "start_time": "2020-05-28T16:29:11.709300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'errorbar.capsize': 3})\n",
    "jones_question6(df_local_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.4\n",
    "The data before the start of the experiment seems to have more heteroskedastic errors regarding the ols estimate of the variance.  After the experiment errors seem fairly consistent across time periods.  This is likely because the individuals who were used to generate the control and treatment groups in the experiment do not represent a random sample of the users in the experiment who were using yelp before January 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you are done!"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python [conda env:econ31340-env] *",
   "language": "python",
   "name": "conda-env-econ31340-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
